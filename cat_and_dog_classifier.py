# -*- coding: utf-8 -*-
"""Cat_and_Dog_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vkbma6wq6bv-KyWplY5WBYKDcA-y7Y09

### Install / Import Libraries
"""

!pip install opendatasets -q

# Import Libraries
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import time
import opendatasets as od

"""### Download data set."""

od.download('https://www.kaggle.com/datasets/dineshpiyasamara/cats-and-dogs-for-classification')

"""### Load Data Sets"""

BATCH_SIZE = 32                # Images per batch.
IMAGE_SIZE = (128,128)         # Image size in pixels.

# File locations of train data and test data.
train_data_dir = '/content/cats-and-dogs-for-classification/cats_dogs/train'
test_data_dir = '/content/cats-and-dogs-for-classification/cats_dogs/test'

# Split data to train and validation.

train_data = tf.keras.utils.image_dataset_from_directory(
                  train_data_dir,
                  batch_size = BATCH_SIZE,
                  image_size = IMAGE_SIZE,
                  seed = 42,
                  validation_split = 0.1,       # 10% data for validation.
                  subset = 'training'   )       # train_data is for 'training'


validation_data = tf.keras.utils.image_dataset_from_directory(
                      train_data_dir,
                      batch_size = BATCH_SIZE,
                      image_size = IMAGE_SIZE,
                      seed = 42,
                      validation_split = 0.1,         # 10% data for validation.
                      subset = 'validation'   )       # train_data is for 'validation'


test_data = tf.keras.utils.image_dataset_from_directory(
                  test_data_dir,
                  batch_size = BATCH_SIZE,
                  image_size = IMAGE_SIZE   )

class_names = train_data.class_names
class_names

"""### Let's display somethings."""

batch1 = train_data.take(1)  # Take 1st batch of train data set.

for image_batch, label_batch in batch1:
  print(image_batch.shape)
  print(label_batch.shape)

for image_batch, label_batch in train_data.take(3):
  print(image_batch.shape)
  print(label_batch.shape)

# Plot data samples.

plt.figure(figsize=(10,4))                        # This line creates a new figure for the plot with a specified size of 10 inches in width and 4 inches in height.
for image,label in train_data.take(1):
  for i in range(10):                             # Iterates from 0 to 9, as you want to plot 10 images.
    ax = plt.subplot(2,5,i+1)                     # Creates a subplot in a 2x5 grid (2 rows and 5 columns) and selects the i+1-th subplot for the current iteration. The subplot index starts from 1.
    plt.imshow(image[i].numpy().astype('uint8'))  # 'image[i]' extracts the i-th image from the batch, and 'numpy()' converts it to a NumPy array, and 'astype('uint8')' ensures that the pixel values are of type unsigned 8-bit integers.
    plt.title(class_names[label[i]])              # 'label[i]' is the label (class index) of the current image, and 'class_names[label[i]]' maps it to the class name.
    plt.axis('off')

for image,label in train_data.take(1):
  print(f'label[6] = {label[6]}')          # label[i] is the index in class_names list.
  print('\n')
  print(image[6])
  print(image[6].numpy().astype('uint8'))

"""### Data preprocessing

##### Data Normalization
"""

for image, label in train_data.take(1):
  print(image[0])

# Normalize feature values.
train_data = train_data.map(lambda x,y:(x/255,y))
validation_data = validation_data.map(lambda x,y:(x/250,y))
test_data = test_data.map(lambda x,y:(x/255,y))

for image, label in train_data.take(1):
  print(image[0])

"""##### Data Augmentation
Here we use a augmentation layer in network.
"""

#Create data augmentation layer.

data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal", input_shape=(128,128,3)),
    tf.keras.layers.RandomRotation(0.2),
    tf.keras.layers.RandomZoom(0.2)
])

data_augmentation.summary()

"""### Model Building"""

model = tf.keras.Sequential()

model.add(data_augmentation)

model.add(tf.keras.layers.Conv2D(32, kernel_size=3, activation='relu'))
model.add(tf.keras.layers.MaxPooling2D())

model.add(tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu'))
model.add(tf.keras.layers.MaxPooling2D())

model.add(tf.keras.layers.Conv2D(128, kernel_size=3, activation='relu'))
model.add(tf.keras.layers.MaxPooling2D())

model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.BatchNormalization())

model.add(tf.keras.layers.Flatten())

model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dense(32, activation='relu'))

model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

model.summary()

"""##### Compile Model"""

# Compile Model
model.compile(
    optimizer = tf.keras.optimizers.Adam(),
    loss = tf.keras.losses.BinaryCrossentropy(),
    metrics = ['accuracy']
)

"""##### Train Model"""

start_time = time.time()

hostory = model.fit( train_data,
                     epochs = 3,
                     validation_data = validation_data )

end_time = time.time()

fig = plt.figure()
plt.plot(hostory.history['accuracy'], color='teal', label='accuracy')
plt.plot(hostory.history['val_accuracy'], color='orange', label='val_accuracy')
fig.suptitle('Accuracy', fontsize=20)
plt.legend()
plt.show()

fig = plt.figure()
plt.plot(hostory.history['loss'], color='teal', label='loss')
plt.plot(hostory.history['val_loss'], color='orange', label='val_loss')
plt.suptitle('Loss', fontsize=20)
plt.legend()
plt.show()

precision = tf.keras.metrics.Precision()
recall = tf.keras.metrics.Recall()
accuracy = tf.keras.metrics.BinaryAccuracy()

for batch in test_data.as_numpy_iterator():
  x, y = batch
  yhat = model.predict(x)
  precision.update_state(y,yhat)
  recall.update_state(y,yhat)
  accuracy.update_state(y,yhat)

print(x.shape)
print(y.shape)

precision.result()

recall.result()

accuracy.result()

"""### Prediction Pipeline"""

import cv2

image = cv2.imread('/content/Predict/dog.JPG')
plt.imshow(image)
plt.show()

resized_image = tf.image.resize(image, IMAGE_SIZE)
scaled_image = resized_image/255

scaled_image

scaled_image.shape

np.expand_dims(scaled_image, 0).shape

plt.imshow(scaled_image)
plt.show()

y_hat = model.predict(np.expand_dims(scaled_image, 0))

print(y_hat)

y_hat.shape

class_names

if y_hat >= 0.5:
  print(class_names[1])
else:
  print(class_names[0])